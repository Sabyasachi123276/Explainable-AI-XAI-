{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning:** Deep learning is a subset of machine learning using artificial neural networks to model and solve complex tasks like image recognition, natural language processing, and predictions, mimicking human decision-making through hierarchical feature learning."
      ],
      "metadata": {
        "id": "vyOX8mYWYiTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN:** A Convolutional Neural Network (CNN) is a type of deep learning model designed for processing grid-like data, such as images. It uses convolutional layers to automatically extract spatial features, pooling layers for dimensionality reduction, and fully connected layers for classification or regression tasks. CNNs excel in image recognition and object detection."
      ],
      "metadata": {
        "id": "65wLmo8-ZSVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Explainability in Deep Learning:** Explainability in deep learning refers to the ability to understand and interpret how a model makes its predictions. It's crucial for building trust, ensuring transparency, and identifying potential biases in models. In sensitive areas like healthcare, finance, or law, explainability helps stakeholders make informed decisions and ensures that the model's behavior aligns with ethical standards and regulatory requirements."
      ],
      "metadata": {
        "id": "gwmdExxFnjA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Attention Mechanism:** An attention mechanism in deep learning is a technique that allows models to focus on specific parts of input data, assigning varying importance to different elements. It enhances performance by prioritizing relevant features, particularly in sequence-based tasks like natural language processing, image captioning, and speech recognition, enabling better context understanding and improved predictions."
      ],
      "metadata": {
        "id": "WTuDejibeq5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN With Attention mechanism:**\n",
        "A Convolutional Neural Network (CNN) with Attention Mechanism combines the powerful feature extraction capabilities of CNNs with the ability of attention mechanisms to prioritize important regions or features in input data. While CNNs excel at learning spatial hierarchies in images, they treat all features equally within a receptive field. The attention mechanism addresses this by dynamically weighting the relevance of different features or regions, enabling the model to focus on the most critical areas.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "**Feature Extraction:** CNN layers extract spatial features from images or other structured data.\n",
        "\n",
        "**Attention Module:** An attention module (e.g., spatial or channel-wise attention) highlights the most informative parts of these features.\n",
        "\n",
        "**Enhanced Understanding:** The integration improves performance, especially in complex tasks requiring nuanced decision-making like image recognition, object detection, or medical image analysis.\n",
        "\n",
        "This combination results in more robust, interpretable, and accurate models."
      ],
      "metadata": {
        "id": "7U6kPm9dzUVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grad-CAM:** Gradient-weighted Class Activation Mapping (Grad-CAM) is a visualization technique that highlights regions in an input image contributing most to a model's prediction. It uses gradients from the final convolutional layer to generate heatmaps, providing interpretability by showing which areas influenced the decision, making deep learning models more transparent and explainable."
      ],
      "metadata": {
        "id": "eQXyIZg_aNxw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps associated with Grad-CAM:**\n",
        "\n",
        "**Forward pass:** The image is passed through the model, and we compute the class score for a target class.\n",
        "\n",
        "**Backward pass:** The gradient with respect to the class score is computed and backpropagated.\n",
        "\n",
        "**Activation and Gradient Handling:** We pool the gradients across all channels of the last convolutional layer. These pooled gradients are used to weight the activations, producing a heatmap that highlights important areas in the image.\n",
        "\n",
        "**Normalization:** The heatmap is normalized to the range [0, 1] and resized to match the input image dimensions."
      ],
      "metadata": {
        "id": "6nUKLWpPawXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importance of Grad-CAM heatmap:** The Grad-CAM heatmap is applied to the image to visualize important regions for the target class prediction. A color overlay is used to enhance the visibility of the regions of interest. The output will be a visualization of the image with the Grad-CAM heatmap superimposed on top of it. The regions of the image that contribute most to the model's prediction will be highlighted in warmer colors (e.g., red and yellow)."
      ],
      "metadata": {
        "id": "41lLkORkpHWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CIFAR-10 dataset description:** The CIFAR-10 dataset is a collection of 60,000 32x32 color images across 10 categories. The 10 categories in the CIFAR-10 dataset are: Airplane, Automobile, Bird, Cat, Deer, Dog, Frog, Horse, Ship and Truck. It is widely used for training machine learning models in image classification tasks, with 50,000 training and 10,000 test images."
      ],
      "metadata": {
        "id": "s5Jg374Alr17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch                        #Imports PyTorch, a framework for deep learning.\n",
        "import torch.nn as nn               #Provides tools for building neural networks (e.g., layers, loss functions).\n",
        "import torch.optim as optim         #Contains optimization algorithms like SGD and Adam.\n",
        "import torchvision                  #Provides utilities for common vision datasets, models, and image transformations.\n",
        "import torchvision.transforms as transforms      #Supports transformations for preprocessing image data.\n",
        "import matplotlib.pyplot as plt                  #Enables plotting and visualization of results.\n",
        "import numpy as np                               #For numerical operations on arrays.\n",
        "import cv2                                       #Provides image processing capabilities (OpenCV).\n",
        "\n",
        "# Define CNN Model with Attention\n",
        "class CNNWithAttention(nn.Module):              #Defines a CNN with an attention mechanism.\n",
        "    def __init__(self):                         #Constructor to define layers and initialize the model.\n",
        "        super(CNNWithAttention, self).__init__()            #super(): Refers to the parent class of the current class (CNNWithAttention).(CNNWithAttention, self):Indicates that we are calling the parent of CNNWithAttention for the current object (self).__init__():Invokes the constructor of the parent class (nn.Module), which sets up the base functionality required for PyTorch modules.\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)         #This line creates a 2D convolutional layer, self.conv1, with 3 input channels, 64 output channels, a kernel size of 3x3, and padding of 1.\n",
        "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)       #This line creates a 2D convolutional layer, self.conv2, with 64 input channels, 128 output channels, a kernel size of 3x3, and padding of 1.\n",
        "        self.attention = AttentionLayer(128)                #Apply attention on the second convolutional layer\n",
        "        self.fc1 = nn.Linear(128*8*8, 512)                  #This line creates a fully connected layer, `self.fc1`, mapping an input of size (128 x 8 x 8) (flattened to a 1D vector of 8192 elements) to an output of 512 features. It's used for feature transformation.\n",
        "        self.fc2 = nn.Linear(512, 10)                       #Final layer mapping 512 units to 10 classes (CIFAR-10 categories)\n",
        "        self.pool = nn.MaxPool2d(2, 2)                      #This line defines a 2D max pooling layer, `self.pool`, that reduces spatial dimensions by half with a 2x2 window.\n",
        "        self.relu = nn.ReLU()                               #Activation function ReLU is introducing non-linearity.\n",
        "\n",
        "        # For storing activations during the forward pass\n",
        "        self.activations = None             #This line initializes the attribute, self.gradients, to None.\n",
        "        self.gradients = None               #This line initializes the attribute, self.activations, to None.\n",
        "\n",
        "        # Register hook to capture activations and gradients\n",
        "        self.conv2.register_forward_hook(self.save_activation)          #Saves feature maps from conv2 during the forward pass\n",
        "        self.conv2.register_backward_hook(self.save_gradient)           #Saves gradients during the backward pass.\n",
        "\n",
        "    def save_activation(self, module, input, output):       #This defines a method `save_activation` that stores the output (activation) of a module. It takes the module, input, and output as arguments, typically used for capturing activations during forward pass for later use.\n",
        "        self.activations = output                           #This assigns the output of a layer or module to the `activations` attribute for later use.\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):   #This defines a method `save_gradient` that stores the gradients during backpropagation. It takes the module, gradient inputs, and gradient outputs as arguments, typically used for capturing gradients for analysis.\n",
        "        self.gradients = grad_output[0]                         #This assigns the first element of the gradient output (`grad_output[0]`) to the `gradients` attribute, typically storing the gradients of the layer's output for later use in analysis.\n",
        "\n",
        "    def forward(self, x):                                   #This defines the forward pass method, `forward`, that processes input `x` through the network layers.\n",
        "        x = self.pool(self.relu(self.conv1(x)))             #This line applies a series of transformations to the input `x`: first, a convolution (`self.conv1`), then a ReLU activation (`self.relu`), followed by a max pooling operation (`self.pool`), which reduces the spatial dimensions of the output.\n",
        "        x = self.pool(self.relu(self.conv2(x)))             #This line applies a series of transformations to the input `x`: first, a convolution (`self.conv2`), then a ReLU activation (`self.relu`), followed by a max pooling operation (`self.pool`), which reduces the spatial dimensions of the output.\n",
        "        x = self.attention(x)  # Apply attention after convolution\n",
        "        x = x.view(-1, 128*8*8)  #This line reshapes the tensor `x` into a 2D tensor with a size of (-1) (automatically determined based on the batch size) and (128 x 8 x 8) features, flattening the spatial dimensions into a single vector per sample.\n",
        "        x = self.relu(self.fc1(x))   #This line applies a fully connected layer (`self.fc1`) to the input `x`, followed by a ReLU activation function (`self.relu`). It transforms `x` through a linear transformation and then introduces non-linearity with ReLU.\n",
        "        x = self.fc2(x)   #This line applies a second fully connected layer (`self.fc2`) to the input `x`, performing a linear transformation that outputs a new tensor, typically for producing the final predictions or feature representation.\n",
        "        return x          #The return x statement outputs the tensor x from the function\n",
        "\n",
        "\n",
        "# Define Attention Layer (for simplicity)\n",
        "class AttentionLayer(nn.Module):      #Defines a class `AttentionLayer` that inherits from PyTorch's `nn.Module`, used for building neural networks.\n",
        "    def __init__(self, in_channels):  #Defines the initialization method for a class, taking `in_channels` as an input parameter.\n",
        "        super(AttentionLayer, self).__init__()    #This line initializes the parent class (`AttentionLayer`) using Python's `super()`, ensuring the base class's constructor runs properly.\n",
        "        self.attention_weights = nn.Parameter(torch.randn(1, in_channels, 1, 1))  #The shape (1, in_channels, 1, 1) means one set of weights per input channel, shared across spatial dimensions, useful in channel-wise attention mechanisms. nn.Parameter makes it trainable during backpropagation.\n",
        "\n",
        "    def forward(self, x):    #defines the forward pass function of a model, specifying how input x is processed through the network.\n",
        "        attention_map = torch.sigmoid(self.attention_weights)  #applies the sigmoid activation function to attention_weights, producing values between 0 and 1, representing the attention map.\n",
        "        return x * attention_map  #apply attention map to input\n",
        "\n",
        "\n",
        "# Grad-CAM Function\n",
        "def generate_gradcam(model, input_image, target_class):     #defines a function that generates Grad-CAM visualizations for a given model, input image, and target class.\n",
        "    model.eval()   #sets the model to evaluation mode, disabling behaviors like dropout and batch normalization.\n",
        "    input_image = input_image.unsqueeze(0)  #adds an extra dimension to `input_image`, typically for batch processing in neural networks.\n",
        "    input_image.requires_grad_()  #enable gradient calculation\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_image)  #passes the input_image through the model to generate predictions.\n",
        "\n",
        "    # Zero all gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backpropagate to get gradients w.r.t the target class\n",
        "    target = output[0][target_class]     #extracts the value from the output tensor corresponding to the target_class index.\n",
        "    target.backward()      #computes the gradients of the target tensor with respect to the model's parameters using backpropagation.\n",
        "\n",
        "    # Get the gradient of the last convolutional layer\n",
        "    gradients = model.gradients            #assign the model's stored gradient to the variables gradients\n",
        "    activations = model.activations        #assign the model's stored activation values to the variables activations\n",
        "\n",
        "    # Pool the gradients across all the channels\n",
        "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])  #The line `pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])` computes the mean of the `gradients` tensor across dimensions 0, 2, and 3, effectively averaging the gradients over the batch, height, and width of the data.\n",
        "\n",
        "    # Weight the activations by the gradients\n",
        "    for i in range(activations.shape[1]):    #starts a loop that iterates over the second dimension of the activations tensor, typically representing the number of channels in the data.\n",
        "        activations[:, i, :, :] *= pooled_gradients[i]  #The line `activations[:, i, :, :] *= pooled_gradients[i]` multiplies each channel of the `activations` tensor (indexed by `i`) by the corresponding value in `pooled_gradients[i]`, scaling the activations based on the gradient values for each channel.\n",
        "\n",
        "    # Take the average of the weighted activations\n",
        "    heatmap = torch.mean(activations, dim=1).squeeze()  #computes the mean of the activations tensor along dimension 1 (usually the channel dimension), then removes any singleton dimensions with .squeeze(), resulting in a simplified heatmap.\n",
        "\n",
        "    # Normalize the heatmap\n",
        "    heatmap = np.maximum(heatmap.detach().numpy(), 0)       #This line converts the heatmap tensor to a NumPy array, detaching it from the computation graph and ensuring all values are non-negative by applying np.maximum to set any negative values to zero.\n",
        "    heatmap = cv2.resize(heatmap, (input_image.shape[2], input_image.shape[3]))  #resizes the heatmap to match the spatial dimensions (width and height) of the input image. Specifically, input_image.shape[2] and input_image.shape[3] represent the width and height of the input_image. The cv2.resize() function adjusts the heatmap to have the same dimensions as the input image, which is typically necessary when overlaying or comparing the heatmap with the original image in tasks like visualization or analysis.\n",
        "    heatmap = heatmap / heatmap.max()  #normalizes the heatmap by dividing each value in the heatmap by the maximum value in the heatmap. This scales the values so that the highest value becomes 1, and all other values fall between 0 and 1.\n",
        "\n",
        "    return heatmap   #The normalized heatmap is then returned, ensuring a consistent range of values for further processing or visualization.\n",
        "\n",
        "\n",
        "# Visualization function\n",
        "def display_gradcam(image, heatmap):          #This defines a function `display_gradcam` that takes an image and a heatmap as inputs to visualize the Grad-CAM result.\n",
        "    image = image.numpy().transpose(1, 2, 0)       #This line converts the image tensor to a NumPy array using .numpy(). The .transpose(1, 2, 0) rearranges the dimensions of the image from (channels, height, width) to (height, width, channels), which is the standard format for displaying images in libraries like Matplotlib.\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)        #This line scales the heatmap to 8-bit integer values (0–255), applies the JET colormap from OpenCV, and creates a colorized heatmap for visualization.\n",
        "    heatmap = np.float32(heatmap) / 255                                      #This line normalizes the heatmap values by converting them to float32 data type and scaling them to the range [0, 1].\n",
        "    overlayed_image = 0.4 * heatmap + 0.6 * image                            #Combines heatmap and original image.\n",
        "    plt.imshow(overlayed_image)                                              #Displays the overlay.\n",
        "    plt.axis('off')                           #Turn off the axis labels\n",
        "    plt.show()                                #Display the plotted image using Matplotlib, ensuring a cleaner, distraction-free visualization.\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':           #This line ensures that the code block below it runs only when the script is executed directly, not imported as a module.\n",
        "    # Load CIFAR-10 dataset\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])   #transforms.ToTensor() converts image data from a PIL Image or NumPy array into a PyTorch tensor, scaling pixel values to the range [0, 1]. transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) standardizes the pixel values for all three RGB channels by subtracting the mean 0.5 and dividing by the standard deviation 0.5. This centers the data around zero with a range [-1, 1].\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)            #This line initializes the CIFAR-10 test dataset. It stores data in the `./data` directory, disables training mode, downloads the dataset if unavailable, and applies the specified `transform` preprocessing pipeline.\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False)                                    #This line creates a DataLoader for the testset, loading data in batches of 4 without shuffling.\n",
        "\n",
        "    # Initialize model and load the first image from the test set\n",
        "    model_with_attention = CNNWithAttention()   #The instantiated object model_with_attention represents the CNN model and contains all the layers, operations, and hooks defined in the CNNWithAttention class.\n",
        "    sample_image, label = testset[0]  # Get first image and label\n",
        "    target_class = label  # Use the class label of the image\n",
        "\n",
        "    # Generate Grad-CAM heatmap\n",
        "    heatmap = generate_gradcam(model_with_attention, sample_image, target_class)\n",
        "\n",
        "    # Display the image with Grad-CAM overlay\n",
        "    display_gradcam(sample_image, heatmap)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "0lvM6ybeBSVJ",
        "outputId": "51c4649a-9727-4e0e-9883-24f2623db08c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
            "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The red and yellow regions represent areas of the image where the deep learning model focuses most strongly to make its prediction. These regions have the highest activations and are considered the most influential for the model's decision-making.\n",
        "The green and blue areas are less influential, indicating lower activation values.\n"
      ],
      "metadata": {
        "id": "9T5y_mp0sQQI"
      }
    }
  ]
}